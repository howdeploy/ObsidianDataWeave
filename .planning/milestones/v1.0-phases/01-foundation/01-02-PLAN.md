---
phase: 01-foundation
plan: 02
type: execute
wave: 2
depends_on:
  - 01-01
files_modified:
  - scripts/fetch_docx.sh
  - scripts/parse_docx.py
autonomous: true
requirements:
  - DOCX-01
  - DOCX-02
  - DOCX-03

must_haves:
  truths:
    - "Running fetch_docx.sh with a Google Drive filename downloads the .docx to /tmp/dw/staging/ and prints the local path"
    - "Running fetch_docx.sh with a non-existent filename prints a clear error and exits non-zero"
    - "Running parse_docx.py on a .docx file produces valid JSON with sections preserving heading hierarchy"
    - "Heading levels in JSON output are normalized (min heading level in doc becomes level 1)"
    - "Inline formatting (bold, italic) is preserved as markdown in paragraphs"
    - "Document is split into sections by headings as input units for atomization"
  artifacts:
    - path: "scripts/fetch_docx.sh"
      provides: "rclone Google Drive fetch with error handling"
      min_lines: 20
    - path: "scripts/parse_docx.py"
      provides: ".docx to structured JSON parser"
      min_lines: 60
      exports: ["parse_docx_to_json"]
  key_links:
    - from: "scripts/fetch_docx.sh"
      to: "config.example.toml"
      via: "reads [rclone] remote from config.toml at runtime"
      pattern: "rclone.*remote"
    - from: "scripts/parse_docx.py"
      to: "/tmp/dw/staging/"
      via: "reads .docx from staging dir, writes JSON to staging dir"
      pattern: "staging"
    - from: "scripts/fetch_docx.sh"
      to: "scripts/parse_docx.py"
      via: "fetch downloads .docx that parse reads"
      pattern: "staging"
---

<objective>
Build the .docx fetch and parse pipeline: a shell script that downloads files from Google Drive via rclone, and a Python parser that converts .docx into structured JSON preserving heading hierarchy.

Purpose: This is the data ingestion pipeline — everything downstream (Phase 2 atomization, Phase 3 vault writing) reads the JSON this plan produces.
Output: scripts/fetch_docx.sh, scripts/parse_docx.py
</objective>

<execution_context>
@/home/kosya/.claude/get-shit-done/workflows/execute-plan.md
@/home/kosya/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-CONTEXT.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create fetch_docx.sh rclone download script</name>
  <files>scripts/fetch_docx.sh</files>
  <action>
Create scripts/fetch_docx.sh — a bash script that downloads a named .docx from Google Drive to local staging.

Requirements:
- Takes filename as first argument (required)
- Reads rclone remote name from config.toml `[rclone] remote` field using a simple grep/sed (no Python dependency for a bash script). Fallback to "gdrive:" if config.toml not found but print a warning.
- Reads staging dir from config.toml `[rclone] staging_dir` field. Fallback to "/tmp/dw/staging".
- Creates staging directory with `mkdir -p`
- Uses `rclone copyto` with list-form arguments (NOT shell=True equivalent — do NOT use eval or unquoted vars)
- On success: prints "Downloaded to: {path}" to stderr and the local file path to stdout (for piping)
- On exit code 3 (file not found): prints clear error to stderr, lists available .docx files from Drive
- On other errors: prints rclone error to stderr
- Uses `set -euo pipefail`
- Make executable with chmod +x

Config reading approach (simple, no Python needed):
```bash
# Read config value: parse_config section key default
parse_config() {
    local file="config.toml"
    if [ ! -f "$file" ]; then
        echo "$3"  # default
        return
    fi
    # Simple TOML extraction (works for flat key = "value" patterns)
    python3 -c "import tomllib; d=tomllib.load(open('$file','rb')); print(d.get('$1',{}).get('$2','$3'))"
}
```

Use the python3 one-liner for reliable TOML parsing (avoids fragile sed/grep on TOML). This is acceptable since Python is required anyway for the parser.

The script should handle filenames with spaces, Cyrillic characters, and colons correctly. Quote all variables. Use list-form for rclone command (each argument separate, no shell expansion).

IMPORTANT: Do NOT hardcode "gdrive:" — read from config.
  </action>
  <verify>
Run: `bash scripts/fetch_docx.sh 2>&1; echo "exit: $?"` (no args) — must show usage error and exit non-zero.

Run: `bash scripts/fetch_docx.sh "nonexistent-file-12345.docx" 2>&1; echo "exit: $?"` — must show error about file not found (exit code 3) or rclone error.

Run: `bash -n scripts/fetch_docx.sh` — syntax check passes.

Run: `test -x scripts/fetch_docx.sh && echo 'executable'` — must print executable.
  </verify>
  <done>fetch_docx.sh downloads named .docx from Google Drive to staging, reads rclone remote from config.toml, handles errors clearly, is executable.</done>
</task>

<task type="auto">
  <name>Task 2: Create parse_docx.py structured JSON parser</name>
  <files>scripts/parse_docx.py</files>
  <action>
Create scripts/parse_docx.py — a Python script that parses a .docx file into structured JSON for downstream atomization.

Use the verified patterns from RESEARCH.md (Pattern 1: Heading-Aware Section Splitting). The research provides working code that was tested against both reference .docx files.

Core requirements:
1. **Heading detection:** Use `paragraph.style.name` with regex `Heading \d+` (case-insensitive). Do NOT assume H1 is top-level — detect min heading level dynamically.
2. **Level normalization:** min_level in document becomes level 1 in output. Emit `heading_depth_offset` for traceability.
3. **Inline formatting:** Convert runs to markdown: bold -> `**text**`, italic -> `*text*`, bold+italic -> `***text***`. Implement `runs_to_markdown(para)` function.
4. **Images:** Mark as `[image]` placeholder in text flow per user decision.
5. **Lists:** Detect list paragraphs (paragraphs with `List Paragraph` style or starting with bullet/dash/number) and preserve as markdown list items. Per Claude's discretion, convert to markdown list syntax within paragraphs array.
6. **Tables:** Use `doc.tables` to extract tables. Convert to markdown table format. Insert at approximate position in sections (after the last paragraph before the table's position). Per Claude's discretion.
7. **Pre-heading content:** Paragraphs before first heading go into a section with `"heading": null, "level": 0`.
8. **Empty paragraphs:** Skip (strip whitespace, skip if empty).

JSON output schema (locked per RESEARCH.md):
```json
{
  "source_file": "filename.docx",
  "heading_depth_offset": N,
  "sections": [
    {
      "heading": "Section Title" | null,
      "level": 1,
      "paragraphs": ["text with **bold** and *italic*..."]
    }
  ]
}
```

CLI interface:
- `python3 scripts/parse_docx.py <input.docx>` — prints JSON to stdout
- `python3 scripts/parse_docx.py <input.docx> -o output.json` — writes to file
- Exit 1 with error message if file not found or not a .docx

Make the `parse_docx_to_json(path)` function importable (use `if __name__ == "__main__"` guard) for Phase 2 integration.

Use `json.dumps(result, ensure_ascii=False, indent=2)` for output (preserves Cyrillic).

Add `argparse` for CLI (stdlib). Keep it simple: positional `input` arg, optional `-o/--output` arg.
  </action>
  <verify>
Test against the actual reference .docx files in project root:

Run: `python3 scripts/parse_docx.py "Архитектура Второго мозга: Синхронизация Obsidian и Claude MCP.docx" | python3 -c "import sys,json; d=json.load(sys.stdin); print(f'sections: {len(d[\"sections\"])}, offset: {d[\"heading_depth_offset\"]}'); assert len(d['sections']) > 5, 'Too few sections'; assert d['heading_depth_offset'] >= 2, 'Should detect H3+ as top level'"` — must show multiple sections with offset >= 2.

Run: `python3 scripts/parse_docx.py "Smart Connections: Интеллектуальный мозг вашей базы Obsidian.docx" | python3 -c "import sys,json; d=json.load(sys.stdin); print(f'sections: {len(d[\"sections\"])}')"` — must show sections > 0.

Run: `python3 scripts/parse_docx.py nonexistent.docx 2>&1; echo "exit: $?"` — must exit 1 with error.

Run: `python3 -c "from scripts.parse_docx import parse_docx_to_json; print(type(parse_docx_to_json))"` — must print `<class 'function'>` (importable).
  </verify>
  <done>parse_docx.py parses both reference .docx files into valid JSON with normalized heading levels, preserves inline markdown formatting, handles Cyrillic, and is importable as a module.</done>
</task>

</tasks>

<verification>
1. `bash scripts/fetch_docx.sh "Smart Connections: Интеллектуальный мозг вашей базы Obsidian.docx"` downloads to staging (live test with real Drive)
2. `python3 scripts/parse_docx.py "Архитектура Второго мозга: Синхронизация Obsidian и Claude MCP.docx" | python3 -m json.tool > /dev/null` — valid JSON
3. JSON output has sections with normalized levels (level 1 = top-level heading, not raw H3)
4. `**bold**` and `*italic*` formatting preserved in paragraph text
5. Both scripts handle Cyrillic filenames without encoding errors
</verification>

<success_criteria>
- fetch_docx.sh downloads a named .docx from Google Drive to /tmp/dw/staging/ reading rclone remote from config.toml
- fetch_docx.sh exits with clear error on file-not-found
- parse_docx.py produces structured JSON with heading hierarchy, normalized levels, and inline markdown
- Both reference .docx files parse successfully with sections > 5
- parse_docx_to_json is importable as a Python function
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md`
</output>
