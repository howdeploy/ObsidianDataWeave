---
phase: 03-writers
plan: "02"
type: execute
wave: 2
depends_on:
  - "03-01"
files_modified:
  - scripts/vault_writer.py
  - scripts/process.py
  - .gitignore
autonomous: true
requirements:
  - VAULT-01
  - VAULT-02
  - VAULT-03
  - VAULT-04

must_haves:
  truths:
    - "vault_writer.py copies .md files from staging to the correct vault subfolders based on note_type"
    - "Running vault_writer.py twice on the same staging dir does not create duplicates — second run prompts or auto-skips"
    - "MOC files go to moc_folder, atomic notes go to notes_folder"
    - "processed.json registry is created on first run and updated after each successful vault write"
    - "process.py chains the full pipeline: fetch -> parse -> atomize -> generate -> write"
    - "MOC is always written last, after all atomic notes"
  artifacts:
    - path: "scripts/vault_writer.py"
      provides: "Staging to vault copier with dedup and folder routing"
      min_lines: 120
      exports: ["main"]
    - path: "scripts/process.py"
      provides: "Full pipeline wrapper"
      min_lines: 50
      exports: ["main"]
  key_links:
    - from: "scripts/vault_writer.py"
      to: "config.toml"
      via: "tomllib.load for vault_path and folder names"
      pattern: "tomllib\\.load"
    - from: "scripts/vault_writer.py"
      to: "processed.json"
      via: "json.loads/json.dumps for registry read/write"
      pattern: "processed\\.json"
    - from: "scripts/vault_writer.py"
      to: "staging/*.md"
      via: "Path.glob and shutil.copy2"
      pattern: "shutil\\.copy2"
    - from: "scripts/process.py"
      to: "scripts/fetch_docx.sh"
      via: "subprocess.run"
      pattern: "fetch_docx"
    - from: "scripts/process.py"
      to: "scripts/parse_docx.py"
      via: "subprocess.run"
      pattern: "parse_docx"
    - from: "scripts/process.py"
      to: "scripts/atomize.py"
      via: "subprocess.run"
      pattern: "atomize"
    - from: "scripts/process.py"
      to: "scripts/generate_notes.py"
      via: "subprocess.run"
      pattern: "generate_notes"
    - from: "scripts/process.py"
      to: "scripts/vault_writer.py"
      via: "subprocess.run"
      pattern: "vault_writer"
---

<objective>
Create vault_writer.py (staging to vault copier with deduplication, folder routing, and interactive conflict resolution) and process.py (full pipeline wrapper). Also add processed.json to .gitignore.

Purpose: This completes the Phase 3 writer pipeline. vault_writer.py is the ONLY component that touches the real Obsidian vault — it reads .md files from staging, checks the processed.json registry for duplicates, prompts the user on conflicts, routes files to the correct vault subfolders by note_type, and writes MOC last. process.py wraps the entire fetch-to-vault chain into a single command.

Output: scripts/vault_writer.py, scripts/process.py, updated .gitignore
</objective>

<execution_context>
@/home/kosya/.claude/get-shit-done/workflows/execute-plan.md
@/home/kosya/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-writers/03-CONTEXT.md
@.planning/phases/03-writers/03-RESEARCH.md
@.planning/phases/03-writers/03-01-SUMMARY.md
@scripts/atomize.py
@scripts/generate_notes.py
@scripts/fetch_docx.sh
@scripts/parse_docx.py
@config.example.toml
@.gitignore
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create vault_writer.py with dedup registry, conflict resolution, and folder routing</name>
  <files>scripts/vault_writer.py</files>
  <action>
Create scripts/vault_writer.py (~160-200 lines) with the following functions and behavior:

**CLI interface:**
- `argparse` with `--staging` (required: staging directory path) and optional `--atom-plan` (atom plan JSON for additional note_type context, but frontmatter is the primary source)
- All diagnostics go to stderr
- Summary printed to both stdout and stderr (stdout for process.py chaining)

**Functions to implement:**

1. `load_config() -> dict` — Read config.toml via tomllib. If config.toml missing: print ERROR to stderr and `sys.exit(1)` (vault_writer MUST know vault_path — unlike generate_notes.py, there is no safe fallback). Same tomllib import pattern as atomize.py.

2. `load_registry() -> dict` — Read processed.json from PROJECT_ROOT. If file does not exist, return `{}` (first run creates it). Never crash on missing registry.

3. `save_registry(registry: dict) -> None` — Write processed.json to PROJECT_ROOT with `ensure_ascii=False, indent=2`. UTF-8 encoding.

4. `parse_frontmatter(content: str) -> dict` — Extract key/value pairs from YAML frontmatter block. Split content by `---`, parse key: value lines from the middle section. Handle tags as list (lines starting with `  - `). Handle double-quoted values (strip quotes). Return dict with keys: tags, date, source_doc, note_type.

5. `resolve_conflict(title: str, source_doc: str) -> str` — Returns "skip" or "overwrite". If `sys.stdin.isatty()` is False: auto-skip with stderr message. Otherwise interactive prompt: `[s]kip / [o]verwrite?`. Default on empty input: skip. Loop until valid input.

6. `get_vault_dest(note_type: str, config: dict) -> Path` — Return vault destination folder. Routing: "atomic" -> vault_path/notes_folder, "moc" -> vault_path/moc_folder, "source" -> vault_path/source_folder. Default to notes_folder for unknown types.

7. `main()` — Load config, load registry, optionally load atom plan for note_type/source_doc maps. Glob staging_dir for `*.md` files. Sort so MOCs are processed LAST (sort key: 1 if stem ends with " — MOC" else 0, then by name). For each .md file:
   - Read content, parse frontmatter for source_doc and note_type
   - If note_type != "moc": check registry for duplicate (source_doc + title in registry[source_doc]["note_titles"])
   - If duplicate found: call resolve_conflict(). On "skip": increment skipped counter, continue. On "overwrite": proceed.
   - If note_type == "moc": always overwrite (per user decision — MOC is auto-generated, no manual edits expected)
   - Determine vault destination via get_vault_dest(), mkdir -p
   - shutil.copy2(md_file, dest_dir / md_file.name)
   - Track for registry update: (source_doc, title, date)
   - After ALL copies complete: update registry atomically (one save_registry call at the end)
   - Print summary to stderr: "Created N notes + M MOC, skipped K duplicates"
   - Print same summary to stdout for process.py

**Dedup key:** (source_doc, title) pair per user decision. NOT content hash.

**Registry schema (processed.json):**
```json
{
  "SourceDoc.docx": {
    "source_doc": "SourceDoc.docx",
    "date": "2026-02-26",
    "note_count": 5,
    "note_titles": ["Title1", "Title2", ...]
  }
}
```

**Critical constraints:**
- vault_writer.py is the ONLY script allowed to write to vault_path (non-negotiable architecture constraint)
- MOC written LAST — sort .md files so note_type=="moc" entries are at the end
- Registry updated AFTER all vault writes complete (atomic update)
- config.toml is REQUIRED for this script (no fallback — vault_path must be known)
  </action>
  <verify>
1. Create a test vault directory: `mkdir -p /tmp/dw/test-vault/Notes /tmp/dw/test-vault/MOCs`
2. Create a temporary config.toml pointing to the test vault (vault_path=/tmp/dw/test-vault, notes_folder=Notes, moc_folder=MOCs, source_folder=Sources)
3. Use the staging .md files from Plan 01's verification (or create mock ones)
4. Run: `python3 scripts/vault_writer.py --staging /tmp/dw/staging`
5. Verify:
   - Exit code 0
   - Atomic notes in /tmp/dw/test-vault/Notes/
   - MOC file in /tmp/dw/test-vault/MOCs/
   - processed.json created in project root with correct entries
   - Run vault_writer.py again — second run should show "skipped N duplicates" (dedup works)
6. Clean up test vault and config
  </verify>
  <done>
vault_writer.py copies staged .md files to the correct vault subfolders based on note_type. Duplicates detected by (source_doc, title) in processed.json registry. Interactive conflict resolution with non-interactive auto-skip fallback. MOC always written last. Registry updated atomically after all writes.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create process.py pipeline wrapper and update .gitignore</name>
  <files>scripts/process.py, .gitignore</files>
  <action>
**Part A: Create scripts/process.py (~70-90 lines)**

Pipeline wrapper that chains: fetch_docx.sh -> parse_docx.py -> atomize.py -> generate_notes.py -> vault_writer.py.

**CLI interface (argparse):**
- Positional: `input` — Google Drive filename (e.g., "Research.docx") or atom plan JSON path
- `--skip-fetch` — Skip fetch step; expect .docx already in staging
- `--from-plan` — Input is an atom plan JSON path; skip fetch/parse/atomize, go straight to generate+write

**Functions:**

1. `run(cmd: list[str], desc: str) -> str` — Run subprocess, capture stdout/stderr. On failure: print stderr and raise SystemExit with descriptive message. On success: print stderr to stderr (pass through diagnostics), return stdout.strip(). This is the core chaining mechanism — each script's stdout becomes the next script's input.

2. `main()` — Parse args. Derive PROJECT_ROOT and scripts dir. Chain steps:
   - If `--from-plan`: skip to step 4 with input as atom_plan_path
   - Step 1 (fetch): `bash scripts/fetch_docx.sh {input}` -> captures local .docx path from stdout. Skipped if `--skip-fetch`.
   - Step 2 (parse): `python3 scripts/parse_docx.py {docx_path} -o {stem}-parsed.json` -> parsed JSON path
   - Step 3 (atomize): `python3 scripts/atomize.py {parsed_json_path}` -> captures atom plan path from stdout
   - Step 4 (generate): `python3 scripts/generate_notes.py {atom_plan_path}` -> captures staging dir from stdout
   - Step 5 (write): `python3 scripts/vault_writer.py --staging {staging_dir} --atom-plan {atom_plan_path}` -> final summary

**Critical constraints:**
- All diagnostic output (progress, warnings) flows through stderr — process.py passes it through
- Only stdout is captured for chaining
- Each step must check previous step's exit code (subprocess.run returncode != 0 -> abort)
- Step 2 parse: derive output path as `{docx_path_stem}-parsed.json` in the same directory as the .docx (staging)
- Pass --atom-plan to vault_writer.py so it has additional note_type context

**Part B: Update .gitignore**

Add `processed.json` to .gitignore (it contains per-user vault data, not suitable for git).
  </action>
  <verify>
1. Verify process.py parses arguments correctly:
   ```
   python3 scripts/process.py --help
   ```
   Should show usage with input, --skip-fetch, --from-plan flags.

2. Verify .gitignore contains `processed.json`:
   ```
   grep "processed.json" .gitignore
   ```

3. If a test atom plan JSON exists from Plan 01 verification, test the --from-plan shortcut:
   ```
   python3 scripts/process.py /tmp/dw/test-atom-plan.json --from-plan
   ```
   This should run generate_notes.py then vault_writer.py (requires config.toml with test vault).

4. Verify process.py uses subprocess (not direct imports) — each script runs as separate process:
   ```
   grep "subprocess" scripts/process.py
   ```
  </verify>
  <done>
process.py chains the full pipeline (fetch -> parse -> atomize -> generate -> write) via subprocess. Supports --skip-fetch and --from-plan shortcuts. Each step's stdout chains to the next. processed.json is gitignored.
  </done>
</task>

</tasks>

<verification>
1. Full pipeline test with --from-plan (uses test atom plan JSON, avoids Claude API call):
   - `python3 scripts/process.py /tmp/dw/test-atom-plan.json --from-plan`
   - Atomic notes appear in test vault Notes/ folder
   - MOC appears in test vault MOCs/ folder
   - processed.json created with correct entries
2. Idempotency test:
   - Run the same command again
   - Second run shows "skipped N duplicates" — no new files created in vault
3. Folder routing verification:
   - `ls /tmp/dw/test-vault/Notes/` shows atomic notes
   - `ls /tmp/dw/test-vault/MOCs/` shows MOC file
   - No files in wrong folders
4. Registry integrity:
   - `cat processed.json` shows source_doc, date, note_count, note_titles
5. `grep "processed.json" .gitignore` confirms it is gitignored
</verification>

<success_criteria>
- vault_writer.py is the sole script touching vault_path (VAULT-04 architecture constraint)
- Notes route to correct vault subfolders by note_type (VAULT-02)
- Duplicate detection by (source_doc, title) prevents re-creation on second run (VAULT-03)
- Notes are saved to configurable vault path from config.toml (VAULT-01)
- process.py wraps the full pipeline in a single command with --skip-fetch and --from-plan shortcuts
- processed.json tracks all processed documents for O(1) dedup lookup
</success_criteria>

<output>
After completion, create `.planning/phases/03-writers/03-02-SUMMARY.md`
</output>
